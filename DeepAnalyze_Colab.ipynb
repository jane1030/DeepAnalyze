{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸš€ DeepAnalyze - Google Colab ä¸€é”®è¿è¡Œç‰ˆ\n",
                "\n",
                "è¿™ä¸ªç¬”è®°æœ¬å¯ä»¥è®©ä½ åœ¨ Google Colab çš„å…è´¹ T4 GPU ä¸Šè¿è¡Œ DeepAnalyze-8B æ¨¡å‹ã€‚\n",
                "\n",
                "### âš ï¸ æ³¨æ„äº‹é¡¹\n",
                "1. è¯·ç¡®ä¿åœ¨èœå•æ  **Runtime (è¿è¡Œæ—¶)** -> **Change runtime type (æ›´æ”¹è¿è¡Œæ—¶ç±»å‹)** ä¸­é€‰æ‹©äº† **T4 GPU**ã€‚\n",
                "2. é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ 16GB æ¨¡å‹ï¼Œçº¦éœ€ 10-15 åˆ†é’Ÿã€‚\n",
                "3. è¿è¡Œæ—¶è¯·ä¿æŒç½‘é¡µå¼€å¯ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. åˆå§‹åŒ–ç¯å¢ƒä¸å®‰è£…ä¾èµ–\n",
                "# @markdown ç‚¹å‡»å·¦ä¾§æ’­æ”¾é”®è¿è¡Œã€‚è¿™æ­¥ä¼šå®‰è£… vLLM å’Œå¿…è¦çš„åº“ã€‚\n",
                "\n",
                "import os\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "def run_command(command):\n",
                "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "    while True:\n",
                "        output = process.stdout.readline()\n",
                "        if output == b'' and process.poll() is not None:\n",
                "            break\n",
                "        if output:\n",
                "            print(output.decode().strip())\n",
                "\n",
                "print(\"æ­£åœ¨å…‹éš†ä»“åº“...\")\n",
                "!git clone https://github.com/jane1030/DeepAnalyze.git\n",
                "%cd DeepAnalyze\n",
                "\n",
                "print(\"æ­£åœ¨å®‰è£…ä¾èµ– (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...\")\n",
                "!pip install -q vllm>=0.6.0 pandas scikit-learn seaborn matplotlib fastapi uvicorn python-multipart requests websockets openai pyngrok nest_asyncio\n",
                "\n",
                "print(\"âœ… ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. ä¸‹è½½ DeepAnalyze-8B æ¨¡å‹\n",
                "# @markdown æˆ‘ä»¬ä½¿ç”¨ 4-bit é‡åŒ–ç‰ˆæœ¬ä»¥é€‚åº” Colab çš„æ˜¾å­˜é™åˆ¶ã€‚\n",
                "\n",
                "import os\n",
                "from huggingface_hub import snapshot_download\n",
                "\n",
                "MODEL_DIR = \"models/DeepAnalyze-8B\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "print(\"æ­£åœ¨ä¸‹è½½æ¨¡å‹... (çº¦ 16GBï¼Œè¯·è€å¿ƒç­‰å¾…)\")\n",
                "# æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¸‹è½½åŸç‰ˆï¼Œä½†åœ¨ Colab ä¸Šè¿è¡Œæ—¶å»ºè®®ä½¿ç”¨é‡åŒ–ç‰ˆ\n",
                "# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è€ƒè™‘æ¢æˆ Qwen2.5-7B-Instruct ç­‰æ›´å°çš„æ¨¡å‹æµ‹è¯•\n",
                "snapshot_download(repo_id=\"RUC-DataLab/DeepAnalyze-8B\", local_dir=MODEL_DIR)\n",
                "\n",
                "print(\"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. å¯åŠ¨æœåŠ¡\n",
                "# @markdown è¿è¡Œåä¼šè¾“å‡ºä¸€ä¸ª public URLï¼Œç‚¹å‡»å³å¯è®¿é—® Web ç•Œé¢ã€‚\n",
                "\n",
                "import threading\n",
                "import time\n",
                "from pyngrok import ngrok\n",
                "import uvicorn\n",
                "import nest_asyncio\n",
                "\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# è®¾ç½® ngrok token (å¯é€‰ï¼Œå¦‚æœä¸è®¾ç½®ï¼Œéš§é“æ—¶é—´æœ‰é™åˆ¶)\n",
                "# ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\")\n",
                "\n",
                "# å¯åŠ¨ vLLM æœåŠ¡ (åå°è¿è¡Œ)\n",
                "print(\"æ­£åœ¨å¯åŠ¨ vLLM æ¨¡å‹æœåŠ¡...\")\n",
                "vllm_cmd = \"python3 -m vllm.entrypoints.openai.api_server --model models/DeepAnalyze-8B --trust-remote-code --port 8000 --gpu-memory-utilization 0.95 --max-model-len 4096 &\"\n",
                "os.system(vllm_cmd)\n",
                "\n",
                "# ç­‰å¾… vLLM å¯åŠ¨\n",
                "time.sleep(60) \n",
                "\n",
                "# å¯åŠ¨åç«¯ API\n",
                "print(\"æ­£åœ¨å¯åŠ¨åç«¯ API...\")\n",
                "os.system(\"nohup python3 demo/backend.py > backend.log 2>&1 &\")\n",
                "\n",
                "# å¯åŠ¨å‰ç«¯ (è¿™é‡Œç®€åŒ–ä¸ºåªå¯åŠ¨åç«¯ API æ¼”ç¤ºï¼Œå®Œæ•´å‰ç«¯éœ€è¦ Node.js ç¯å¢ƒ)\n",
                "# Colab ä¸Šè·‘ Next.js å‰ç«¯æ¯”è¾ƒéº»çƒ¦ï¼Œé€šå¸¸æˆ‘ä»¬åªè·‘åç«¯ APIï¼Œæˆ–è€…ç”¨ Gradio é‡å†™ä¸€ä¸ªç®€å•çš„ç•Œé¢\n",
                "\n",
                "# å»ºç«‹å…¬ç½‘éš§é“\n",
                "public_url = ngrok.connect(8200).public_url\n",
                "print(f\"ğŸš€ æœåŠ¡å·²å¯åŠ¨ï¼è¯·è®¿é—®: {public_url}\")\n",
                "\n",
                "# ä¿æŒè¿è¡Œ\n",
                "while True:\n",
                "    time.sleep(10)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}